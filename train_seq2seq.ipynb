{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DICT dataset...\n",
      "Loading the Dataset into the container...\n",
      "Field names: ['en', 'ch', 'pinyin_str', 'pinyin_char']\n",
      "Data sizes: [(Train, 46620), (Valid, 5828), (Test, 5828)]\n",
      "The first example of the training data is:\n",
      "en :  ['k', 'a', 'r', 't', 'm', 'a', 'n']\n",
      "ch :  ['卡', '特', '曼']\n",
      "pinyin_str :  ['ka', 'te', 'man']\n",
      "pinyin_char :  ['k', 'a', 't', 'e', 'm', 'a', 'n']\n",
      "Loading NEWS dataset...\n",
      "Loading the Dataset into the container...\n",
      "Field names: ['en', 'ch', 'pinyin_str', 'pinyin_char']\n",
      "Data sizes: [(Train, 81252), (Valid, 513), (Test, 1000)]\n",
      "The first example of the training data is:\n",
      "en :  ['k', 'u', 's', 'i', 'c', 'k']\n",
      "ch :  ['库', '西', '克']\n",
      "pinyin_str :  ['ku', 'xi', 'ke']\n",
      "pinyin_char :  ['k', 'u', 'x', 'i', 'k', 'e']\n"
     ]
    }
   ],
   "source": [
    "from mnmt.encoder import BasicEncoder\n",
    "from mnmt.decoder import BasicFeedForwardDecoder\n",
    "from mnmt.decoder import GreedyDecoder\n",
    "from mnmt.attention import AdditiveAttention\n",
    "from mnmt.decoder import BridgeLayer\n",
    "from mnmt.model import Seq2Seq\n",
    "from mnmt.datasets import *\n",
    "from mnmt.inputter import ArgsFeeder\n",
    "from mnmt.inputter import ModuleArgsFeeder\n",
    "from mnmt.trainer.utils import *\n",
    "from mnmt.trainer import Trainer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current device for PyTorch is cuda\n",
      "Seq2Seq(\n",
      "  (encoder): BasicEncoder(\n",
      "    (embedding): Sequential(\n",
      "      (0): Embedding(33, 256)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): GreedyDecoder(\n",
      "    (feed_forward_decoder): BasicFeedForwardDecoder(\n",
      "      (attention): AdditiveAttention(\n",
      "        (additive_mapping): Linear(in_features=1536, out_features=512, bias=True)\n",
      "        (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "      )\n",
      "      (embedding): Sequential(\n",
      "        (0): Embedding(631, 256)\n",
      "        (1): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (rnn): LSTM(1280, 512, num_layers=2, dropout=0.2)\n",
      "      (prediction): Sequential(\n",
      "        (0): Linear(in_features=1792, out_features=631, bias=True)\n",
      "        (1): LogSoftmax()\n",
      "      )\n",
      "    )\n",
      "    (bridge_layer): BridgeLayer(\n",
      "      (bridge_layer): ModuleList(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "The model has 18,367,351 trainable parameters\n",
      "Running single-main-task experiment...\n",
      "[VAL]: The number of correct predictions (main-task (single)): 341/513\n",
      "[TEST]: The number of correct predictions (main-task (single)): 724/1000\n",
      "           Loss       ACC  ACC-ACT Replaced   ACC+\n",
      "Valid  1.751180  0.664717  0.68616   11/163     NA\n",
      "Test   0.666981  0.724000  0.74200   18/254  0.733\n"
     ]
    }
   ],
   "source": [
    "def set_up_args(data_container, exp_num):\n",
    "    build_vocabs(data_container, dict_min_freqs={'en': 1, 'ch': 1, 'pinyin_str': 1, 'pinyin_char': 1})\n",
    "    for name, field in data_container.fields:\n",
    "        if name == 'en':\n",
    "            input_dim = len(field.vocab)\n",
    "            src_pad_idx = field.vocab.stoi[field.pad_token]\n",
    "        elif name == 'ch':\n",
    "            output_dim = len(field.vocab)\n",
    "            trg_pad_idx = field.vocab[field.pad_token]\n",
    "\n",
    "    enc_args_feeder = ModuleArgsFeeder(input_dim=input_dim, embedding_dim=256, hidden_dim=512,\n",
    "                                       embedding_dropout=0.1, rnn_type='LSTM',\n",
    "                                       num_layers=2, rnn_dropout=0.2)\n",
    "    dec_args_feeder = ModuleArgsFeeder(input_dim=output_dim, embedding_dim=256, hidden_dim=512,\n",
    "                                       embedding_dropout=0.1, rnn_type='LSTM',\n",
    "                                       num_layers=2, rnn_dropout=0.2)\n",
    "    return ArgsFeeder(enc_args_feeder, [dec_args_feeder],\n",
    "                      batch_size=64, src_pad_idx=src_pad_idx, trg_pad_idx=trg_pad_idx,\n",
    "                      optim_choice='Adam', learning_rate=0.003, decay_patience=0,\n",
    "                      lr_decay_factor=0.9, valid_criterion='ACC', early_stopping_patience=100,\n",
    "                      total_epochs=100, report_interval=50, exp_num=exp_num, multi_task_ratio=1, data_container=data_container,\n",
    "                      src_lang='en', trg_lang='ch', auxiliary_name='pinyin_str', quiet_translate=True,\n",
    "                      valid_out_path=f\"experiments/exp{exp_num}/valid.out\", test_out_path=f\"experiments/exp{exp_num}/test.out\")\n",
    "\n",
    "\n",
    "def test_seq2seq(args_feeder):\n",
    "    decoder_args_feeder = args_feeder.decoder_args_feeders[0]\n",
    "    encoder = BasicEncoder(args_feeder)\n",
    "    feed_forward_decoder = \\\n",
    "        BasicFeedForwardDecoder(args_feeder,\n",
    "                                AdditiveAttention(encoder_hidden_dim=args_feeder.encoder_args_feeder.hidden_dim,\n",
    "                                                  decoder_hidden_dim=decoder_args_feeder.hidden_dim), decoder_index=0)\n",
    "    bridge_layer = BridgeLayer(encoder_hidden_dim=args_feeder.encoder_args_feeder.hidden_dim,\n",
    "                               decoder_hidden_dim=decoder_args_feeder.hidden_dim,\n",
    "                               num_of_states=2)\n",
    "    decoder = GreedyDecoder(feed_forward_decoder, bridge_layer, device=args_feeder.device)\n",
    "    model = Seq2Seq(args_feeder, encoder, decoder, teacher_forcing_ratio=0.8).to(args_feeder.device)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_reproducibility(seed=1234)\n",
    "    dict_dataset = DICT['data_container']\n",
    "    news_dataset = NEWS['data_container']\n",
    "    \n",
    "    try:\n",
    "        # DICT\n",
    "        seq2seq_args_feeder = set_up_args(dict_dataset, exp_num=666)\n",
    "        test_model = test_seq2seq(seq2seq_args_feeder)\n",
    "        test_trainer = Trainer(seq2seq_args_feeder, test_model)\n",
    "        test_trainer.run(burning_epoch=0)\n",
    "        test_trainer.best_model_output()\n",
    "\n",
    "        # NEWS\n",
    "        seq2seq_args_feeder = set_up_args(news_dataset, exp_num=667)\n",
    "        test_model = test_seq2seq(seq2seq_args_feeder)\n",
    "        test_trainer = Trainer(seq2seq_args_feeder, test_model)\n",
    "        # test_trainer.run(burning_epoch=10)\n",
    "        test_trainer.best_model_output(test_ref_dict=NEWS['test-set-dict'])\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
